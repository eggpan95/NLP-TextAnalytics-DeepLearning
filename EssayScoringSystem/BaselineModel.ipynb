{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training_set_rel3.tsv\", encoding='iso-8859-1', delimiter='\\t')  ## error with utf-8 encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score      ...        \\\n",
       "0             NaN             NaN            NaN      ...         \n",
       "1             NaN             NaN            NaN      ...         \n",
       "2             NaN             NaN            NaN      ...         \n",
       "3             NaN             NaN            NaN      ...         \n",
       "4             NaN             NaN            NaN      ...         \n",
       "\n",
       "   rater2_trait3  rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a look how an essay content looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"essay\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that there are few external references like \"@CAPS2\" and rest of the message seems almost clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Meta Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to analyze the each essay to understand what features are required for the model. By using TextBlob library package\n",
    "     \n",
    "     1) Grammar Mistakes\n",
    "     2) Essay length\n",
    "     3) Average Word length\n",
    "     4) Average Sentence Length\n",
    "     5) Essay Sentiment\n",
    "     6) Spelling Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    message = TextBlob(data[\"essay\"][i])\n",
    "    \n",
    "    #number of words\n",
    "    data.set_value(i,'Essay_Length',len(message.words))\n",
    "    \n",
    "    #Spelling Mistakes\n",
    "    try:\n",
    "        dictinoray = enchant.Dict('en_US')\n",
    "        vocab_words = character_filter(message.words)\n",
    "        checks = [dictionary.check(word) for word in vocab_words]\n",
    "        data.set_value(i,'Spelling_Mistakes', checks.count(False))\n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    #number of grammar mistakes\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    matches = tool.check(data[\"essay\"][i])\n",
    "    data.set_value(i,'Grammer_Mistakes',len(matches))\n",
    "    \n",
    "    #average word length\n",
    "    len_word = [len(word) for word in message.words]\n",
    "    data.set_value(i,'Average_Word_Length',sum(len_word)/len(len_word))\n",
    "    \n",
    "    #sentence length\n",
    "    length = [len(sentence.split(' ')) for sentence in message.sentences]\n",
    "    data.set_value(i,'Sentence_Length',sum(length)/len(length))\n",
    "    \n",
    "    #sentiment of each essay\n",
    "    data.set_value(i,'Sentiment',message.sentiment.polarity)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    343.0\n",
       "1    422.0\n",
       "2    283.0\n",
       "3    527.0\n",
       "4    470.0\n",
       "Name: Essay_Length, dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Essay_Length\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of the Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is not required, just for a demonstration purpose\n",
    "import tokenize\n",
    "from nltk import word_tokenize\n",
    "text = word_tokenize(data[\"essay\"][0])\n",
    "#tags = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step,\n",
    "     \n",
    "    1) Converting essays to a bag of words models (remove stop words, create bag of words and vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a stemmer widely used\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using seperate countvectorize functions for each essay type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df = [data[data['essay_set']==i+1] for i in range(8)]\n",
    "# Turn into vector of features\n",
    "vectorizers = [CountVectorizer( analyzer = 'word',\n",
    "                                tokenizer = tokenize,\n",
    "                                lowercase = True,\n",
    "                                stop_words = 'english') for i in range(8)]\n",
    "corpuses = [df[i]['essay'].values for i in range(8)]\n",
    "word_mats = [vectorizers[i].fit_transform(corpuses[i]) for i in range(8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 10929)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_mats[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "        \n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "transformed_data = [tfidf_transformer.fit_transform(word_mats[i]) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1800x9324 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 165093 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing target variable (score) for each essay type before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "dfs = [data[data['essay_set']==i+1] for i in range(8)]\n",
    "scores = [dfs[i]['domain1_score'] for i in range(8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase, this problem deals with estimiating the scores for each essay, supervised regression would be an apt approach rather than a \n",
    "multi-label classification. The float values obtained in the regression can be rounded off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to run a baseline model to understand the initial performance. Rather than using mean square error (mse)or R2, \n",
    "I am also using Spearman's rank correlation cofficient that finds how closely the predicted scores correspond to true scores - \n",
    "this measures the the strength and direction of monotonic association between the essay feature and the score. In other words, \n",
    "it determines how well the ranking of the features corresponds with the ranking of the scores. The benefit of this approach is that this is a useful measure for grading essays, since we're interested to know how directly a feature predicts the relative score of an essay (i.e., how an essay compares to another essay) rather than the actual score given to the essay. Ultimately, this is a better model to measure rather than accuracy, since it gives direct insight into the influence of the feature on the score, and furthermore, because relative accuracy might be more important than actual accuracy.\n",
    "\n",
    "Spearman results in a score ranging from -1 to 1, where the closer the score is to an absolute value of 1, the stronger the monotonic association (and where positive values imply a positive monotonic association, versus negative values implying a negative one). The closer the value to 0, the weaker the monotonic association. The general consensus of Spearman correlation strength interpretation is as follows:\n",
    "\n",
    ".00-.19 “very weak”\n",
    ".20-.39 “weak”\n",
    ".40-.59 “moderate”\n",
    ".60-.79 “strong”\n",
    ".80-1.0 “very strong”\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "https://github.com/kevinloughlin/Automated-Essay-Grading/tree/master/Readings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for essay_id 0\n",
      "         mse        r2                                spear\n",
      "GB  1.143185  0.556585   (0.754679511523, 1.3274507625e-50)\n",
      "LR  3.194124 -0.238928  (0.491222659269, 1.10205105729e-17)\n",
      "RF  1.442985  0.440299  (0.660707115005, 5.40047594696e-35)\n",
      "Results for essay_id 1\n",
      "         mse        r2                                spear\n",
      "GB  0.336200  0.460642  (0.684770458602, 1.05337804183e-38)\n",
      "LR  0.895703 -0.436957  (0.343596936548, 6.74589589157e-09)\n",
      "RF  0.382704  0.386037  (0.604202326807, 2.94896482983e-28)\n",
      "Results for essay_id 2\n",
      "         mse        r2                                spear\n",
      "GB  0.373325  0.422866  (0.659416612237, 1.07207734296e-33)\n",
      "LR  1.629326 -1.518825   (0.186755896542, 0.00254779015039)\n",
      "RF  0.391390  0.394938  (0.640274399504, 2.79829117192e-31)\n",
      "Results for essay_id 3\n",
      "         mse        r2                                spear\n",
      "GB  0.353174  0.605532    (0.79404853459, 5.1642349841e-59)\n",
      "LR  4.243786 -3.739985  (0.365834302503, 7.59886678367e-10)\n",
      "RF  0.452049  0.495096  (0.719441382371, 1.10274000447e-43)\n",
      "Results for essay_id 4\n",
      "         mse        r2                                spear\n",
      "GB  0.279543  0.697359  (0.852363154242, 1.05025413847e-77)\n",
      "LR  5.278449 -4.714585  (0.251402721353, 2.82763850258e-05)\n",
      "RF  0.421181  0.544019  (0.759843705791, 3.14395601025e-52)\n",
      "Results for essay_id 5\n",
      "         mse        r2                                spear\n",
      "GB  0.264065  0.720689  (0.820031668816, 6.60254701027e-67)\n",
      "LR  2.684850 -1.839854  (0.246051224049, 4.36446013485e-05)\n",
      "RF  0.334889  0.645777  (0.765960015601, 2.41234667207e-53)\n",
      "Results for essay_id 6\n",
      "          mse        r2                                spear\n",
      "GB  10.093559  0.514236  (0.735318514988, 2.06474480976e-41)\n",
      "LR  24.233834 -0.166282  (0.508791760291, 6.06621706965e-17)\n",
      "RF  13.514068  0.349619   (0.597935873114, 2.8573503462e-24)\n",
      "Results for essay_id 7\n",
      "          mse        r2                               spear\n",
      "GB  13.617520  0.425507   (0.6211606993, 5.75357994218e-13)\n",
      "LR  43.272084 -0.825552   (0.199290964105, 0.0377499081958)\n",
      "RF  18.625138  0.214247  (0.46352587776, 3.85850257837e-07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Same process will be repeated for each essay set\n",
    "for i in range(8):\n",
    "    #85:15 ratio - train test split\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        transformed_data[i], \n",
    "        scores[i],\n",
    "        train_size=0.85, \n",
    "        random_state=1234)\n",
    "\n",
    "\n",
    "    # Fit the Regressors using the training dataset\n",
    "    Regressors =  {\"GB\": {\"f\": GradientBoostingRegressor()},\n",
    "                   \"RF\": {\"f\": RandomForestRegressor()},\n",
    "                   \"LR\": {\"f\": LinearRegression()}}\n",
    "\n",
    "    for model in Regressors.keys():\n",
    "        # Fit\n",
    "        Regressors[model][\"f\"].fit(X_train, y_train)\n",
    "        # Predict\n",
    "        Regressors[model][\"c\"] = Regressors[model][\"f\"].predict(X_test.toarray())\n",
    "    \n",
    "    #Evaluate\n",
    "    measures = {\"mse\": mean_squared_error, \"r2\": r2_score,\"spear\":spearmanr}\n",
    "\n",
    "    results = pd.DataFrame(columns=measures.keys())\n",
    "\n",
    "    # Evaluate each model in Regressors\n",
    "    for model in Regressors.keys():\n",
    "        results.loc[model] = [measures[measure](y_test, Regressors[model][\"c\"]) for measure in measures.keys()]\n",
    "    \n",
    "    print (\"Results for essay_id {}\".format(i))\n",
    "    print (results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Interpretation:\n",
    "     \n",
    "    Spearman's values:\n",
    "    \n",
    "    Each tuple represents the Spearman Scores followed by \"p\" values.\n",
    "    \n",
    "    \n",
    "    As we can see from the results, that the GradientBoosting Regressor(has stronger correlation of spearman's rank) outperforms the Linear Regression and Randomforest regression.\n",
    "    We can further use grid parameter search to apply the combination of parameters to find the best parameters. The stacking and ensembling\n",
    "    of algorithms will improve the results. Moreover, a deep neural network can be applied to check if the performance is improved\n",
    "                                                                        \n",
    "    A further research is required to find the best features for the essay 6 and essay 7.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
